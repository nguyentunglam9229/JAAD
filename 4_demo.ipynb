{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4_demo.ipynb","provenance":[],"collapsed_sections":["ZgFvexv0I-cO","3dhetbgvfM-k","6oeYG6X6veMh","uW7BmYZwz22g"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WztHVeuggOAn","executionInfo":{"status":"ok","timestamp":1611374834113,"user_tz":-420,"elapsed":799,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}},"outputId":"f5db2023-019a-4e39-d916-1172b959edaf"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sat Jan 23 04:07:13 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87ewwVKUR8Lz","executionInfo":{"status":"ok","timestamp":1611374663458,"user_tz":-420,"elapsed":19935,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}},"outputId":"db00b86a-526e-4ac2-ab51-4e5a80f5af06"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-AW9aXydIP5T","executionInfo":{"status":"ok","timestamp":1611374837803,"user_tz":-420,"elapsed":773,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["import os\r\n","os.chdir('/content/drive/MyDrive/NLP_and_CV_Projects/CV/Model/YOWO/')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgTiZ6wCe4bn","executionInfo":{"status":"ok","timestamp":1611375942890,"user_tz":-420,"elapsed":816,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["num_vid = 5\r\n","testlist     = '/content/drive/MyDrive/NLP_and_CV_Projects/CV/data/annotations_yowo/testlist_000'+str(num_vid)+'.txt'"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"YqgM95DBfotW","executionInfo":{"status":"ok","timestamp":1611375943218,"user_tz":-420,"elapsed":966,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}},"outputId":"bde08d01-1f26-490c-d4e4-8d1e3a72bd4f"},"source":["testlist"],"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/NLP_and_CV_Projects/CV/data/annotations_yowo/testlist_0005.txt'"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"markdown","metadata":{"id":"3dhetbgvfM-k"},"source":["# Import YOWO"]},{"cell_type":"code","metadata":{"id":"kBdWCFHEJQys","executionInfo":{"status":"ok","timestamp":1611375945032,"user_tz":-420,"elapsed":854,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["import os\r\n","os.chdir('/content/drive/MyDrive/NLP_and_CV_Projects/CV/Model/YOWO/')"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzkVd6abbCLh","executionInfo":{"status":"ok","timestamp":1611375945355,"user_tz":-420,"elapsed":1172,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","import numpy as np\r\n","import torch.nn.functional as F\r\n","from torch.autograd import Variable\r\n","\r\n","from cfg import *\r\n","from cfam import CFAMBlock\r\n","from backbones_2d import darknet\r\n","from backbones_3d import mobilenet, shufflenet, mobilenetv2, shufflenetv2, resnext, resnet\r\n","\r\n","\r\n","\r\n","num_classes = 2 # 24 is the number of classes of ucf101 dataset, \r\n","#if we set this differently, we need to specify to not import pretrained weights of the last layer\r\n","\r\n","\r\n","\r\n","class YOWO(nn.Module): # model.py\r\n","    def __init__(self):\r\n","        super(YOWO, self).__init__()\r\n","        \r\n","        ##### 2D Backbone #####\r\n","        # if opt.backbone_2d == \"darknet\":\r\n","        self.backbone_2d = darknet.Darknet(\"cfg/yolo.cfg\")\r\n","        num_ch_2d = 425 # Number of output channels for backbone_2d\r\n","        # load pretrained weights\r\n","        self.backbone_2d.load_weights(\"weights/yolo.weights\")\r\n","\r\n","        ##### 3D Backbone ##### # resnet18: lighter but no pre-trained\r\n","        # default = \"resnext101\":\r\n","        self.backbone_3d = resnext.resnext101()\r\n","        num_ch_3d = 2048 # Number of output channels for backbone_3d\r\n","        # load 3d pretrained weights\r\n","        self.backbone_3d = self.backbone_3d.cuda()\r\n","        self.backbone_3d = nn.DataParallel(self.backbone_3d, device_ids=None) # Because the pretrained backbone models are saved in Dataparalled mode\r\n","        pretrained_3d_backbone = torch.load('weights/resnext-101-kinetics.pth')\r\n","        backbone_3d_dict = self.backbone_3d.state_dict()\r\n","        pretrained_3d_backbone_dict = {k: v for k, v in pretrained_3d_backbone['state_dict'].items() if k in backbone_3d_dict} # 1. filter out unnecessary keys\r\n","        backbone_3d_dict.update(pretrained_3d_backbone_dict) # 2. overwrite entries in the existing state dict\r\n","        self.backbone_3d.load_state_dict(backbone_3d_dict) # 3. load the new state dict\r\n","        self.backbone_3d = self.backbone_3d.module # remove the dataparallel wrapper\r\n","\r\n","\r\n","        ##### Attention & Final Conv #####f\r\n","        self.cfam = CFAMBlock(num_ch_2d+num_ch_3d, 1024)\r\n","        self.conv_final = nn.Conv2d(1024, 5*(num_classes+4+1), kernel_size=1, bias=False) # 5: number of anchors, num_classes, 4: coordinates, 1: confidence score\r\n","        self.seen = 0\r\n","\r\n","\r\n","\r\n","    def forward(self, input):\r\n","        x_3d = input # Input clip (None, num_channels, num_frames, W, H) W and H must be divisible to 32 (480, 256?)\r\n","        x_2d = input[:, :, -1, :, :] # Last frame of the clip that is read\r\n","\r\n","        x_2d = self.backbone_2d(x_2d)\r\n","        x_3d = self.backbone_3d(x_3d)\r\n","        x_3d = torch.squeeze(x_3d, dim=2)\r\n","\r\n","        x = torch.cat((x_3d, x_2d), dim=1)\r\n","        x = self.cfam(x)\r\n","\r\n","        out = self.conv_final(x)\r\n","\r\n","        return out\r\n","\r\n","\r\n","def get_fine_tuning_parameters(model, freeze_backbone_2d = False, freeze_backbone_3d = False):\r\n","    ft_module_names = ['cfam', 'conv_final'] # Always fine tune 'cfam' and 'conv_final'\r\n","    if not freeze_backbone_2d:\r\n","        ft_module_names.append('backbone_2d') # Fine tune complete backbone_2d\r\n","    else:\r\n","        ft_module_names.append('backbone_2d.models.29') # Fine tune only layer 29 and 30\r\n","        ft_module_names.append('backbone_2d.models.30') # Fine tune only layer 29 and 30\r\n","\r\n","    if not freeze_backbone_3d:\r\n","        ft_module_names.append('backbone_3d') # Fine tune complete backbone_3d\r\n","    else:\r\n","        ft_module_names.append('backbone_3d.layer4') # Fine tune only layer 4\r\n","\r\n","    parameters = [] # trainable parameteres\r\n","    for k, v in model.named_parameters():\r\n","        for ft_module in ft_module_names:\r\n","            if ft_module in k:\r\n","                parameters.append({'params': v})\r\n","                break\r\n","            else:\r\n","                parameters.append({'params': v, 'lr': 0.0}) # freeze layers\r\n","    \r\n","    return parameters"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"id":"wb_-ZG-zvtLP","executionInfo":{"status":"ok","timestamp":1611375945355,"user_tz":-420,"elapsed":1168,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["# model       = YOWO()\r\n","# model       = model.cuda()\r\n","# model       = nn.DataParallel(model, device_ids=None) # in multi-gpu case\r\n","# model.seen  = 0\r\n","# # print(model)\r\n","\r\n","# parameters = get_fine_tuning_parameters(model)#, opt)\r\n","# print(parameters)\r\n","# optimizer = optim.SGD(model.parameters(), lr=learning_rate/batch_size, momentum=momentum, dampening=0, weight_decay=decay*batch_size)"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"jmXxZ5cxymnd","executionInfo":{"status":"ok","timestamp":1611375945356,"user_tz":-420,"elapsed":1166,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["# count = 0\r\n","# weights1 = []\r\n","# for v in model.parameters():\r\n","#   if count < 5:\r\n","#     weights1.append(v)\r\n","#   count += 1\r\n","# weights2 = []\r\n","# count = 0\r\n","# for k, v in model.named_parameters():\r\n","#   if count <5:\r\n","#     weights2.append(v)\r\n","#   count += 1\r\n","# print(weights1[0]==weights2[0])"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNHvUYJqwZLm","executionInfo":{"status":"ok","timestamp":1611375945356,"user_tz":-420,"elapsed":1162,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["# count = 0\r\n","# for k, v in model.named_parameters():\r\n","#   if count < 2:\r\n","#     print('k = ', k)\r\n","#     print('v = ', v)\r\n","#   count += 1"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"kKoXMqOAZt3A","executionInfo":{"status":"ok","timestamp":1611375945356,"user_tz":-420,"elapsed":1159,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["# model       = YOWO()\r\n","# model       = model.cuda()\r\n","# model       = nn.DataParallel(model, device_ids=None) # in multi-gpu case\r\n","# print(model)"],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ubsge7JnSmK8","executionInfo":{"status":"ok","timestamp":1611375945357,"user_tz":-420,"elapsed":1155,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["# model_state = torch.load('./backup/yowo_ucf101-24_16f_best.pth') # load pre-trained model on ucf101 dataset"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"id":"KYEePC7MhAKX","executionInfo":{"status":"ok","timestamp":1611375945357,"user_tz":-420,"elapsed":1152,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["# model.load_state_dict(model_state['state_dict']) # "],"execution_count":75,"outputs":[]},{"cell_type":"code","metadata":{"id":"07SM-895dzx6","executionInfo":{"status":"ok","timestamp":1611375945357,"user_tz":-420,"elapsed":1149,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["# from torchsummary import summary\r\n","# summary(model, (3, 16,480,256)) # channel x frames x W x H 480x256"],"execution_count":76,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6oeYG6X6veMh"},"source":["# Setup and train function"]},{"cell_type":"code","metadata":{"id":"NdJ3c389vfHs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611375950580,"user_tz":-420,"elapsed":5052,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}},"outputId":"e29c9369-6119-420b-c582-65e829d5da6b"},"source":["from __future__ import print_function\r\n","import sys\r\n","import time\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import torch.optim as optim\r\n","import torch.backends.cudnn as cudnn\r\n","from torchvision import datasets, transforms\r\n","\r\n","import dataset\r\n","import random\r\n","import math\r\n","import os\r\n","# from opts import parse_opts\r\n","from utils import *\r\n","from cfg import parse_cfg\r\n","from region_loss import RegionLoss\r\n","\r\n","# from model import YOWO, get_fine_tuning_parameters\r\n","\r\n","# # Training settings\r\n","# opt = parse_opts()\r\n","# # which dataset to use\r\n","# dataset_use   = opt.dataset\r\n","\r\n","dataset_use = 'jaad'\r\n","\r\n","# assert dataset_use == 'ucf101-24' or dataset_use == 'jhmdb-21', 'invalid dataset'\r\n","# # path for dataset of training and validation\r\n","# datacfg       = opt.data_cfg\r\n","# # path for cfg file\r\n","# cfgfile       = opt.cfg_file\r\n","\r\n","# data_options  = read_data_cfg(datacfg)\r\n","# net_options   = parse_cfg(cfgfile)[0]\r\n","\r\n","# obtain list for training and testing\r\n","# Demo\r\n","basepath = '/content/drive/MyDrive/NLP_and_CV_Projects/CV/data'\r\n","trainlist     = '/content/drive/MyDrive/NLP_and_CV_Projects/CV/data/trainlist.txt' # /content/data/trainlist.txt\r\n","# testlist      = '/content/drive/MyDrive/NLP_and_CV_Projects/CV/data/annotations_yowo/testlistdemo.txt'# /content/data/testlist.txt\r\n","testlist = testlist\r\n","\r\n","backupdir     = '/content/drive/MyDrive/NLP_and_CV_Projects/CV/Model/YOWO/backup'\r\n","# number of training samples\r\n","nsamples      = file_lines(trainlist)\r\n","gpus          = '0' #data_options['gpus']  # e.g. 0   ,1,2,3\r\n","ngpus         = len(gpus.split(','))\r\n","num_workers   = 4  #?int(data_options['num_workers']) 0, 4, 10?\r\n","\r\n","batch_size    = 8        #int(net_options['batch']) # 12\r\n","clip_duration = 16        #int(net_options['clip_duration']) # 16 frames\r\n","max_batches   = 200000    #int(net_options['max_batches']) # 100000\r\n","learning_rate = 0.001    #float(net_options['learning_rate']) # 0.0001\r\n","momentum      = 0.9       # float(net_options['momentum']) # 0.9\r\n","decay         = 0.0005    #float(net_options['decay']) # 0.0005\r\n","steps         = [10000,20000,30000,40000]   # [float(step) for step in net_options['steps'].split(',')] # [10000,20000,30000,40000] or [30000,40000,50000,60000]\r\n","scales        = [0.5,0.5,0.5,0.5]           #[float(scale) for scale in net_options['scales'].split(',')] # [0.5,0.5,0.5,0.5]\r\n","\r\n","# loss parameters\r\n","# loss_options               = parse_cfg(cfgfile)[1]\r\n","region_loss                = RegionLoss()\r\n","# anchors                    = loss_options['anchors'].split(',')\r\n","region_loss.anchors        = [0.70458, 1.18803, 1.26654, 2.55121, 1.59382, 4.08321, 2.30548, 4.94180, 3.52332, 5.91979]\r\n","#[float(i) for i in anchors] #anchors = 0.70458, 1.18803, 1.26654, 2.55121, 1.59382, 4.08321, 2.30548, 4.94180, 3.52332, 5.91979\r\n","region_loss.num_classes    = 2   # int(loss_options['classes']) # 24\r\n","region_loss.num_anchors    = 5    #int(loss_options['num']) # 5\r\n","region_loss.anchor_step    = len(region_loss.anchors)//region_loss.num_anchors #\r\n","region_loss.object_scale   = 5. # float(loss_options['object_scale']) # 5\r\n","region_loss.noobject_scale = 1. # float(loss_options['noobject_scale']) # 1 \r\n","region_loss.class_scale    = 1. # float(loss_options['class_scale']) # 1\r\n","region_loss.coord_scale    = 100. # float(loss_options['coord_scale']) # 1\r\n","region_loss.batch          = batch_size #batch_size # 12\r\n","\r\n","\r\n","\r\n","#Train parameters\r\n","max_epochs    = max_batches*batch_size//nsamples+1\r\n","use_cuda      = True #True\r\n","seed          = int(time.time())\r\n","# seed          = 42\r\n","eps           = 1e-5\r\n","best_fscore   = 0   # initialize best fscore\r\n","begin_epoch = 1\r\n","end_epoch = 25\r\n","\r\n","\r\n","# Test parameters\r\n","nms_thresh    = 0.2 # non_maximum suppression threshold 0.4\r\n","iou_thresh    = 0.3 # >=iou threshold => correctly identified 0.5\r\n","# proposals when confidence score >= 0.25\r\n","\r\n","# # print('N_samples = ', nsamples)\r\n","# print('num of batches per epoch =', nsamples//batch_size+1)\r\n","\r\n","# print('nms_thresh =', nms_thresh)\r\n","# print('iou_thresh =', iou_thresh)\r\n","\r\n","if not os.path.exists(backupdir):\r\n","    os.mkdir(backupdir)\r\n","    \r\n","torch.manual_seed(seed)\r\n","if use_cuda:\r\n","    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\r\n","    torch.cuda.manual_seed(seed)\r\n","\r\n","# Create model\r\n","# model = YOWO(opt)\r\n","model       = YOWO()\r\n","model       = model.cuda()\r\n","model       = nn.DataParallel(model, device_ids=None) # in multi-gpu case\r\n","model.seen  = 0\r\n","# print(model)\r\n","\r\n","# parameters = get_fine_tuning_parameters(model)#, opt)\r\n","# optimizer = optim.SGD(parameters, lr=learning_rate/batch_size, momentum=momentum, dampening=0, weight_decay=decay*batch_size)\r\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate/batch_size, momentum=momentum, dampening=0, weight_decay=decay*batch_size)\r\n","\r\n","kwargs = {'num_workers': num_workers, 'pin_memory': True} if use_cuda else {}\r\n","\r\n","resume_path = '/content/drive/MyDrive/NLP_and_CV_Projects/CV/Model/YOWO/backup/yowo_jaad_manual_tune5_coord100_ob25_e4_16f_checkpoint.pth'\r\n","\r\n","# Load resume path if necessary\r\n","# if opt.resume_path:\r\n","if os.path.exists(resume_path):\r\n","    print(\"===================================================================\")\r\n","    print('loading checkpoint {}'.format(resume_path))\r\n","    checkpoint = torch.load(resume_path) # opt.resume_path\r\n","    # opt.begin_epoch = checkpoint['epoch'] + 1\r\n","    begin_epoch = checkpoint['epoch'] + 1\r\n","    best_fscore = checkpoint['fscore']\r\n","    model.load_state_dict(checkpoint['state_dict'])\r\n","    optimizer.load_state_dict(checkpoint['optimizer'])\r\n","    model.seen = checkpoint['epoch'] * nsamples\r\n","    print(\"Loaded model fscore: \", checkpoint['fscore'])\r\n","    print(\"===================================================================\")\r\n","\r\n","region_loss.seen  = model.seen\r\n","processed_batches = model.seen//batch_size\r\n","\r\n","init_width        = 480 # int(net_options['width']) # 224 480 480\r\n","init_height       = 256 # int(net_options['height']) # 224 270 256\r\n","init_epoch        = model.seen//nsamples \r\n","\r\n","##########################\r\n","\r\n","def adjust_learning_rate(optimizer, batch):\r\n","    lr = learning_rate\r\n","    for i in range(len(steps)):\r\n","        scale = scales[i] if i < len(scales) else 1\r\n","        if batch >= steps[i]:\r\n","            lr = lr * scale\r\n","            if batch == steps[i]:\r\n","                break\r\n","        else:\r\n","            break\r\n","    for param_group in optimizer.param_groups:\r\n","        param_group['lr'] = lr/batch_size\r\n","    return lr\r\n","\r\n","\r\n","\r\n","def train(epoch):\r\n","    global processed_batches\r\n","    t0 = time.time()\r\n","    cur_model = model.module\r\n","    region_loss.l_x.reset()\r\n","    region_loss.l_y.reset()\r\n","    region_loss.l_w.reset()\r\n","    region_loss.l_h.reset()\r\n","    region_loss.l_conf.reset()\r\n","    region_loss.l_cls.reset()\r\n","    region_loss.l_total.reset()\r\n","\r\n","    train_loader = torch.utils.data.DataLoader(\r\n","        dataset.listDataset(basepath, trainlist, dataset_use=dataset_use, shape=(init_width, init_height),\r\n","                       shuffle=True,\r\n","                       transform=transforms.Compose([\r\n","                           transforms.ToTensor(),\r\n","                       ]), \r\n","                       train=True, \r\n","                       seen=cur_model.seen,\r\n","                       batch_size=batch_size,\r\n","                       clip_duration=clip_duration,\r\n","                       num_workers=num_workers),\r\n","        batch_size=batch_size, shuffle=False, **kwargs)\r\n","\r\n","    lr = adjust_learning_rate(optimizer, processed_batches)\r\n","    logging('training at epoch %d, lr %f' % (epoch, lr))\r\n","\r\n","    model.train()\r\n","\r\n","    for batch_idx, (data, target) in enumerate(train_loader):\r\n","        # print(data.size)\r\n","        # print(target.size)\r\n","        adjust_learning_rate(optimizer, processed_batches)\r\n","        processed_batches = processed_batches + 1\r\n","\r\n","        if use_cuda:\r\n","            data = data.cuda()\r\n","\r\n","        optimizer.zero_grad()\r\n","        output = model(data)\r\n","        region_loss.seen = region_loss.seen + data.data.size(0)\r\n","        loss = region_loss(output, target)\r\n","        loss.backward()\r\n","        optimizer.step()\r\n","\r\n","        # save result every 500 batches\r\n","        if processed_batches % 500 == 0: # From time to time, reset averagemeters to see improvements\r\n","            region_loss.l_x.reset()\r\n","            region_loss.l_y.reset()\r\n","            region_loss.l_w.reset()\r\n","            region_loss.l_h.reset()\r\n","            region_loss.l_conf.reset()\r\n","            region_loss.l_cls.reset()\r\n","            region_loss.l_total.reset()\r\n","\r\n","    t1 = time.time()\r\n","    logging('trained with %f samples/s' % (len(train_loader.dataset)/(t1-t0)))\r\n","    print('')\r\n"],"execution_count":77,"outputs":[{"output_type":"stream","text":["===================================================================\n","loading checkpoint /content/drive/MyDrive/NLP_and_CV_Projects/CV/Model/YOWO/backup/yowo_jaad_manual_tune5_coord100_ob25_e4_16f_checkpoint.pth\n","Loaded model fscore:  0\n","===================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XWfL0HqJ0ZJQ","executionInfo":{"status":"ok","timestamp":1611375950581,"user_tz":-420,"elapsed":5048,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}},"outputId":"687cb272-642d-4022-bc51-c5563cc88882"},"source":["print('epoch of checkpoint =',checkpoint['epoch'])"],"execution_count":78,"outputs":[{"output_type":"stream","text":["epoch of checkpoint = 4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uW7BmYZwz22g"},"source":["# Test function"]},{"cell_type":"code","metadata":{"id":"uBnTcqckzft1","executionInfo":{"status":"ok","timestamp":1611375950581,"user_tz":-420,"elapsed":2709,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}}},"source":["def test(epoch):\r\n","    def truths_length(truths):\r\n","        for i in range(50):\r\n","            if truths[i][1] == 0:\r\n","                return i\r\n","\r\n","    test_loader = torch.utils.data.DataLoader(\r\n","    dataset.listDataset(basepath, testlist, dataset_use=dataset_use, shape=(init_width, init_height),\r\n","                   shuffle=False,\r\n","                   transform=transforms.Compose([\r\n","                       transforms.ToTensor()\r\n","                   ]), train=False),\r\n","    batch_size=batch_size, shuffle=False, **kwargs)\r\n","\r\n","    num_classes = region_loss.num_classes\r\n","    anchors     = region_loss.anchors\r\n","    num_anchors = region_loss.num_anchors\r\n","    conf_thresh_valid = 0.005\r\n","    total       = 0.0\r\n","    proposals   = 0.0\r\n","    correct     = 0.0\r\n","    fscore = 0.0\r\n","\r\n","    correct_classification = 0.0\r\n","    total_detected = 0.0\r\n","\r\n","    nbatch      = file_lines(testlist) // batch_size\r\n","\r\n","    logging('validation at epoch %d' % (epoch))\r\n","    model.eval()\r\n","\r\n","    for batch_idx, (frame_idx, data, target) in enumerate(test_loader):\r\n","        if use_cuda:\r\n","            data = data.cuda()\r\n","        with torch.no_grad():\r\n","            output = model(data).data\r\n","            all_boxes = get_region_boxes(output, conf_thresh_valid, num_classes, anchors, num_anchors, 0, 1)\r\n","            #output (batch size, anchor*(4+1+num_classes), h, w) ; h, w: grid size\r\n","            for i in range(output.size(0)): # size(0): batch size\r\n","                boxes = all_boxes[i]\r\n","                boxes = nms(boxes, nms_thresh)\r\n","                if dataset_use == 'ucf101-24':\r\n","                    detection_path = os.path.join('ucf_detections', 'detections_'+str(epoch), frame_idx[i])\r\n","                    current_dir = os.path.join('ucf_detections', 'detections_'+str(epoch))\r\n","                    if not os.path.exists('ucf_detections'):\r\n","                        os.mkdir('ucf_detections')\r\n","                    if not os.path.exists(current_dir):\r\n","                        os.mkdir(current_dir)\r\n","                elif dataset_use == 'jaad': \r\n","                    detection_path = os.path.join('jaad_detections_demo', 'detections'+str(num_vid), frame_idx[i])\r\n","                    current_dir = os.path.join('jaad_detections_demo', 'detections'+str(num_vid))\r\n","                    if not os.path.exists('jaad_detections_demo'):\r\n","                        os.mkdir('jaad_detections_demo')\r\n","                    if not os.path.exists(current_dir):\r\n","                        os.mkdir(current_dir)\r\n","                else:\r\n","                    detection_path = os.path.join(dataset_use+'_detections', 'detections_'+str(epoch), frame_idx[i])\r\n","                    current_dir = os.path.join(dataset_use+'_detections', 'detections_'+str(epoch))\r\n","                    if not os.path.exists(dataset_use+'_detections'):\r\n","                        os.mkdir(dataset_use+'_detections')\r\n","                    if not os.path.exists(current_dir):\r\n","                        os.mkdir(current_dir)\r\n","                # print all the boxes\r\n","                # boxes = n_boxes * [x_center, y_center, w, h, confidence score, prob of the activity, int of class predicted]\r\n","                with open(detection_path, 'w+') as f_detect:\r\n","                    for box in boxes:\r\n","                        x1 = round(float(box[0]-box[2]/2.0) * 320.0)\r\n","                        y1 = round(float(box[1]-box[3]/2.0) * 240.0)\r\n","                        x2 = round(float(box[0]+box[2]/2.0) * 320.0)\r\n","                        y2 = round(float(box[1]+box[3]/2.0) * 240.0)\r\n","\r\n","                        det_conf = float(box[4])  # detected confidence > 0.25 is detected\r\n","                        for j in range((len(box)-5)//2):\r\n","                            cls_conf = float(box[5+2*j].item()) # class confidence\r\n","\r\n","                            if type(box[6+2*j]) == torch.Tensor:\r\n","                                cls_id = int(box[6+2*j].item())\r\n","                            else:\r\n","                                cls_id = int(box[6+2*j])\r\n","                            prob = det_conf * cls_conf  # means = detected confidence * class confidence\r\n","\r\n","                            # f_detect.write(str(int(box[6])+1) + ' ' + str(prob) + ' ' + str(x1) + ' ' + str(y1) + ' ' + str(x2) + ' ' + str(y2) + '\\n')\r\n","                            f_detect.write(str(int(box[6])+1) + ' ' + str(det_conf) + ' ' + str(cls_conf) + ' ' + str(x1) + ' ' + str(y1) + ' ' + str(x2) + ' ' + str(y2) + '\\n')\r\n","                \r\n","                truths = target[i].view(-1, 5)\r\n","                num_gts = truths_length(truths)\r\n","        \r\n","                total = total + num_gts  # total ground truths\r\n","    \r\n","                for i in range(len(boxes)): # for all proposal boxes in that frame\r\n","                    if boxes[i][4] > 0.25:\r\n","                        proposals = proposals+1\r\n","\r\n","                for i in range(num_gts):\r\n","                    box_gt = [truths[i][1], truths[i][2], truths[i][3], truths[i][4], 1.0, 1.0, truths[i][0]]\r\n","                    best_iou = 0\r\n","                    best_j = -1\r\n","                    for j in range(len(boxes)): # find the best box (highest iou)\r\n","                        iou = bbox_iou(box_gt, boxes[j], x1y1x2y2=False)\r\n","                        if iou > best_iou:\r\n","                            best_j = j\r\n","                            best_iou = iou\r\n","\r\n","                    if best_iou > iou_thresh:\r\n","                        total_detected += 1\r\n","                        if int(boxes[best_j][6]) == box_gt[6]: # correctly classified\r\n","                            correct_classification += 1\r\n","\r\n","                    if best_iou > iou_thresh and int(boxes[best_j][6]) == box_gt[6]: # correct localization AND classification\r\n","                        correct = correct+1\r\n","\r\n","            precision = 1.0*correct/(proposals+eps) # how accurate is the prediction\r\n","            recall = 1.0*correct/(total+eps)  # correctly localized AND classify\r\n","            fscore = 2.0*precision*recall/(precision+recall+eps)\r\n","            logging(\"[%d/%d] precision: %f, recall: %f, fscore: %f\" % (batch_idx, nbatch, precision, recall, fscore))\r\n","\r\n","    classification_accuracy = 1.0 * correct_classification / (total_detected + eps)  # accuracy of the classification if detected\r\n","    localization_recall = 1.0 * total_detected / (total + eps) # correct localization\r\n","\r\n","    print(\"Classification accuracy: %.3f\" % classification_accuracy)\r\n","    print(\"Localization recall: %.3f\" % localization_recall)\r\n","\r\n","    return classification_accuracy, localization_recall, precision, recall, fscore"],"execution_count":79,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ory1MOqz8c-"},"source":["# Main: Print demo results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"boIG35KGKqT2","executionInfo":{"status":"ok","timestamp":1611376078301,"user_tz":-420,"elapsed":123330,"user":{"displayName":"Toni Vu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6uIu6Vcgzta1ovHLLOvVj5RVWGexlxO7F_N4c=s64","userId":"18061882157352491166"}},"outputId":"658d0368-52f9-4af2-ee84-1d88b4cd2101"},"source":["test(0)"],"execution_count":80,"outputs":[{"output_type":"stream","text":["2021-01-23 04:25:55 validation at epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/13ujwaotnxl8JQ_LapEAWcXISoadgLan6/NLP_and_CV_Projects/CV/Model/YOWO/utils.py:280: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  cls_confs = torch.nn.Softmax()(Variable(output[5:5+num_classes].transpose(0,1))).data #softmax_class\n"],"name":"stderr"},{"output_type":"stream","text":["2021-01-23 04:26:06 [0/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:26:12 [1/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:26:19 [2/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:26:24 [3/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:26:30 [4/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:26:37 [5/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:26:45 [6/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:26:53 [7/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:02 [8/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:10 [9/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:18 [10/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:26 [11/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:33 [12/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:37 [13/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:40 [14/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:44 [15/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:48 [16/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:52 [17/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:57 [18/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","2021-01-23 04:27:57 [19/19] precision: 0.000000, recall: 0.000000, fscore: 0.000000\n","Classification accuracy: 0.000\n","Localization recall: 0.000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(0.0, 0.0, 0.0, 0.0, 0.0)"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"NXcLKkhIBHTu"},"source":["'''\r\n","correct = iou>0.5 (location) AND class is correct\r\n","proposals = conf_score > 0.25\r\n","\r\n","precision = correct/proposals, if I propose a box+class with confidence > 0.25, is it correct? \r\n","recall = correct/total_ground_truths\r\n","fscore\r\n","\r\n","Classification accuracy = classify_correct/total_detected, classification accuracy given detected\r\n","Localization recall = total_detected/total_gr_truths\r\n","'''"],"execution_count":null,"outputs":[]}]}